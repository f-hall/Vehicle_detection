{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# importing all necessary packages\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "import glob\n",
    "from moviepy.editor import VideoFileClip\n",
    "import math\n",
    "import random\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import pylab\n",
    "import imageio\n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skimage.feature import hog\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.ndimage.measurements import label\n",
    "\n",
    "import multiprocessing\n",
    "from multiprocessing import Process\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setting procentage of the data used for the testset (20% here)\n",
    "# The parameter follow is the size of the image-chunks we are getting from the gti-vehicles folder to solve the problem\n",
    "# with the rowing of the data. (For example: the pictures 42-47 are really similar, so we want them in the same set (test or training))\n",
    "# Another easier solution would be just taking 20% in a row from the folder, but that is not random enough for my liking.\n",
    "test_set_percent = 20\n",
    "follow = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting pathes for vehicle-images\n",
    "veh_kitti = sorted(glob.glob('/media/frank/Zusatz/CarND-Vehicle-Detection-master/vehicles/KITTI_extracted/*.png'))\n",
    "veh_gtifar = sorted(glob.glob('/media/frank/Zusatz/CarND-Vehicle-Detection-master/vehicles/GTI_Far/*.png'))\n",
    "veh_gticlose = sorted(glob.glob('/media/frank/Zusatz/CarND-Vehicle-Detection-master/vehicles/GTI_MiddleClose/*.png'))\n",
    "veh_gtiright = sorted(glob.glob('/media/frank/Zusatz/CarND-Vehicle-Detection-master/vehicles/GTI_Right/*.png'))\n",
    "veh_gtileft = sorted(glob.glob('/media/frank/Zusatz/CarND-Vehicle-Detection-master/vehicles/GTI_Left/*.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting pathes for non-vehicle-images\n",
    "non_extras = sorted(glob.glob('/media/frank/Zusatz/CarND-Vehicle-Detection-master/non-vehicles/Extras/*.png'))\n",
    "non_gti = sorted(glob.glob('/media/frank/Zusatz/CarND-Vehicle-Detection-master/non-vehicles/GTI/*.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function for selecting test-sets with the follow-parameter (only necessary for gti-vehicles)\n",
    "def selecting_sets(vehicle_list = [], follow=10, test_set_procent=10):\n",
    "    jo = int(len(vehicle_list)*(test_set_procent/100))\n",
    "    random_list = []\n",
    "    test_list = []\n",
    "    train_list = []\n",
    "    while len(random_list) < jo:\n",
    "        test = random.randint(0, len(vehicle_list)-1-follow)\n",
    "        while test in random_list:\n",
    "            test = random.randint(0, len(vehicle_list)-1-follow)\n",
    "        for i in range(follow):\n",
    "            if test+i not in random_list:\n",
    "                random_list.append(test+i)\n",
    "            \n",
    "    while len(random_list) > jo:\n",
    "        random_list.pop()\n",
    "        \n",
    "    for i in range(len(vehicle_list)):\n",
    "        if i in random_list:\n",
    "            test_list.append(vehicle_list[i])\n",
    "        else:\n",
    "            train_list.append(vehicle_list[i])\n",
    "            \n",
    "    random.shuffle(train_list)\n",
    "    random.shuffle(test_list)\n",
    "    return(train_list, test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Splitting all folder-sets in training and testdata (vehicles)\n",
    "train_far, test_far = selecting_sets(veh_gtifar, follow, test_set_percent)\n",
    "train_close, test_close = selecting_sets(veh_gticlose, follow, test_set_percent)\n",
    "train_right, test_right = selecting_sets(veh_gtiright, follow, test_set_percent)\n",
    "train_left, test_left = selecting_sets(veh_gtileft, follow, test_set_percent)\n",
    "kitti_help = int(len(veh_kitti)*(test_set_percent/100))\n",
    "random.shuffle(veh_kitti)\n",
    "train_kitti, test_kitti = veh_kitti[kitti_help:], veh_kitti[:kitti_help]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Splitting all folder-sets in training and testdata (non-vehicles)\n",
    "gti_help = int(len(non_gti)*(test_set_percent/100))\n",
    "random.shuffle(non_gti)\n",
    "train_gti, test_gti = non_gti[gti_help:], non_gti[:gti_help]\n",
    "\n",
    "extras_help = int(len(non_extras)*(test_set_percent/100))\n",
    "random.shuffle(non_extras)\n",
    "train_extras, test_extras = non_extras[extras_help:], non_extras[:extras_help]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fusion the training and testdata to one set each and shuffling them\n",
    "test_vehicle = test_far + test_close + test_right + test_left + test_kitti\n",
    "train_vehicle = train_far + train_close + train_right + train_left + train_kitti\n",
    "test_non = test_gti + test_extras\n",
    "train_non = train_gti + train_extras\n",
    "\n",
    "random.shuffle(test_vehicle)\n",
    "random.shuffle(train_vehicle)\n",
    "random.shuffle(test_non)\n",
    "random.shuffle(train_non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to get hog features of an image with choosen orient, pix per cell and cell per block\n",
    "# (From udacity-lessons)\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, \n",
    "                        vis=False, feature_vec=True):\n",
    "    if vis == True:\n",
    "        features, hog_image = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block=(cell_per_block, cell_per_block), transform_sqrt=True, \n",
    "                                  visualise=vis, feature_vector=feature_vec)\n",
    "        return features, hog_image\n",
    "    else:      \n",
    "        features = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                       cells_per_block=(cell_per_block, cell_per_block), transform_sqrt=True, \n",
    "                       visualise=vis, feature_vector=feature_vec)\n",
    "        return features\n",
    "# function to extract hog features from an image\n",
    "# (From udacity-lessons)\n",
    "def extract_features(imgs, cspace='RGB', orient=9, \n",
    "                        pix_per_cell=8, cell_per_block=2, hog_channel=0):\n",
    "    features = []\n",
    "    for file in imgs:\n",
    "        image = mpimg.imread(file)\n",
    "        if file.endswith('.png'):\n",
    "            image = cv2.convertScaleAbs(image*255)\n",
    "        if cspace != 'RGB':\n",
    "            if cspace == 'HSV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "            elif cspace == 'LUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2LUV)\n",
    "            elif cspace == 'HLS':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "            elif cspace == 'YUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
    "            elif cspace == 'YCrCb':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n",
    "        else: feature_image = np.copy(image)      \n",
    "\n",
    "        if hog_channel == 'ALL':\n",
    "            hog_features = []\n",
    "            for channel in range(feature_image.shape[2]):\n",
    "                hog_features.append(get_hog_features(feature_image[:,:,channel], \n",
    "                                    orient, pix_per_cell, cell_per_block, \n",
    "                                    vis=False, feature_vec=True))\n",
    "            hog_features = np.ravel(hog_features)        \n",
    "        else:\n",
    "            hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \n",
    "                        pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "        features.append(hog_features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Choose variables for hog-extraction\n",
    "# This involves a lot of trial and error, but I came to the conclusion that this parameters\n",
    "# works best for me.\n",
    "colorspace = 'YCrCb' # Colorspace HLS/YUV were nearly as good\n",
    "orient = 12 # Tested also with 9, but I think 12 is a bit more stable\n",
    "pix_per_cell = 8\n",
    "cell_per_block = 2\n",
    "hog_channel = 'ALL' # Did not test a lot with just 1 Channel. We should take all information we can get here, I suppose.\n",
    "\n",
    "# Extract hog-features for the vehicle-data\n",
    "veh_features = extract_features(train_vehicle, cspace=colorspace, orient=orient, \n",
    "                        pix_per_cell=pix_per_cell, cell_per_block=cell_per_block, \n",
    "                        hog_channel=hog_channel)\n",
    "\n",
    "# Extract hog-features for the non-vehicle-data\n",
    "non_features = extract_features(train_non, cspace=colorspace, orient=orient, \n",
    "                        pix_per_cell=pix_per_cell, cell_per_block=cell_per_block, \n",
    "                        hog_channel=hog_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Getting the labels ready for training data\n",
    "y = np.hstack((np.ones(len(veh_features)), \n",
    "              np.zeros(len(non_features))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting feature vector ready for training data\n",
    "x = np.asarray(veh_features + non_features).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalizing the training data\n",
    "# It's important in theorie that you normalize using just the training data (not the testset) unlike in the udacity-lessons.\n",
    "# But in the end it should not matter much, I suppose.\n",
    "X_scaler = StandardScaler().fit(x)\n",
    "scaled_x = X_scaler.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use a SVM to get a classifier (using our feature-vector scaled_x and label-vector y)\n",
    "# The standard parameters for svm.SVC seems really good (tried a bit with other kernels and Cs)\n",
    "clf1 = svm.SVC()\n",
    "clf1.fit(scaled_x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Getting everything ready for the testset like before with the trainingset\n",
    "veh_features_test = extract_features(test_vehicle, cspace=colorspace, orient=orient, \n",
    "                        pix_per_cell=pix_per_cell, cell_per_block=cell_per_block, \n",
    "                        hog_channel=hog_channel)\n",
    "\n",
    "non_features_test = extract_features(test_non, cspace=colorspace, orient=orient, \n",
    "                        pix_per_cell=pix_per_cell, cell_per_block=cell_per_block, \n",
    "                        hog_channel=hog_channel)\n",
    "\n",
    "y_test = np.hstack((np.ones(len(veh_features_test)), \n",
    "              np.zeros(len(non_features_test))))\n",
    "\n",
    "x_test = np.asarray(veh_features_test + non_features_test).astype(np.float64)\n",
    "\n",
    "scaled_x_test = X_scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Checking the test accuracy of our Classifier (Something around 98.5% - 99.5% most of the time)\n",
    "print('Test Accuracy of SVC = ', round(clf1.score(scaled_x_test, y_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to find cars on an image using hog-features and the classifier\n",
    "# (Mostly taken from udacity-lessons. I commented of changed lines)\n",
    "def find_cars(img, ystart, ystop, scale, svc, X_scaler, orient, pix_per_cell, cell_per_block, steps):\n",
    "    voidlist = []\n",
    "    draw_img = np.zeros_like(img)\n",
    "    \n",
    "    # For small scales I use a smaller search window, as small vehicles will not be near the bottom of the image\n",
    "    if scale <= 1.80:\n",
    "        ystop = 530\n",
    "    \n",
    "    if scale <= 1:\n",
    "        ystop = 500\n",
    "    \n",
    "    img_tosearch = img[ystart:ystop,:,:]\n",
    "    ctrans_tosearch = cv2.cvtColor(img_tosearch, cv2.COLOR_RGB2YCrCb)\n",
    "\n",
    "    if scale != 1:\n",
    "        imshape = ctrans_tosearch.shape\n",
    "        ctrans_tosearch = cv2.resize(ctrans_tosearch, (np.int(imshape[1]/scale), np.int(imshape[0]/scale)))\n",
    "        \n",
    "    ch1 = ctrans_tosearch[:,:,0]\n",
    "    ch2 = ctrans_tosearch[:,:,1]\n",
    "    ch3 = ctrans_tosearch[:,:,2]\n",
    "\n",
    "    # Define blocks and steps as above\n",
    "    nxblocks = (ch1.shape[1] // pix_per_cell)-1\n",
    "    nyblocks = (ch1.shape[0] // pix_per_cell)-1 \n",
    "    nfeat_per_block = orient*cell_per_block**2\n",
    "\n",
    "    # 64 was the orginal sampling rate, with 8 cells and 8 pix per cell\n",
    "    window = 64\n",
    "    nblocks_per_window = (window // pix_per_cell)-1 \n",
    "    # I choose the steps in x-direction fixed as 1. For testing purposes I used the parameter steps for the y-direction.\n",
    "    # Lastly I came to the conclusion that steps=1 for all scales is the most reliable choice albeit needing longer to compute. \n",
    "    cells_per_stepx = 1\n",
    "    cells_per_stepy = steps  # Instead of overlap, define how many cells to step\n",
    "    if scale >= 2:\n",
    "        cells_per_step1 = 1\n",
    "    nxsteps = (nxblocks - nblocks_per_window) // cells_per_step1\n",
    "    nysteps = (nyblocks - nblocks_per_window) // cells_per_step\n",
    "    \n",
    "    # Compute individual channel HOG features for the entire image\n",
    "    hog1 = get_hog_features(ch1, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    hog2 = get_hog_features(ch2, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    hog3 = get_hog_features(ch3, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    \n",
    "    for xb in range(nxsteps):\n",
    "        for yb in range(nysteps):\n",
    "            ypos = yb*cells_per_step\n",
    "            xpos = xb*cells_per_step1\n",
    "            # Extract HOG for this patch\n",
    "            hog_feat1 = hog1[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_feat2 = hog2[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_feat3 = hog3[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_features = np.hstack((hog_feat1, hog_feat2, hog_feat3))\n",
    "\n",
    "            xleft = xpos*pix_per_cell\n",
    "            ytop = ypos*pix_per_cell\n",
    "\n",
    "            # Extract the image patch\n",
    "            subimg = cv2.resize(ctrans_tosearch[ytop:ytop+window, xleft:xleft+window], (64,64))\n",
    "          \n",
    "            test_features = X_scaler.transform(hog_features).ravel()\n",
    "            test_prediction = svc.predict(test_features)\n",
    "             \n",
    "            if test_prediction == 1:\n",
    "                xbox_left = np.int(xleft*scale)\n",
    "                ytop_draw = np.int(ytop*scale)\n",
    "                win_draw = np.int(window*scale) \n",
    "                help_here = np.array([xbox_left, ytop_draw+ystart, xbox_left+win_draw, ytop_draw+win_draw+ystart])\n",
    "                voidlist.append(help_here)\n",
    "                \n",
    "    # Returning all rectangles, where vehicles where found in a list.            \n",
    "    return voidlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General pipeline using find_cars for differet sizes and steps, saving the output in one new list.\n",
    "def find_cars_range(image):\n",
    "    patch_list = []\n",
    "    for i in range(6):\n",
    "        size = [1, 1.5, 1.75, 2, 2.5, 3]\n",
    "        steps = [1, 1, 1, 1, 1, 1]\n",
    "        patch_list.extend(find_cars(image, ystart, ystop, size[i], clf1, X_scaler, orient, pix_per_cell, cell_per_block, steps[i]))#, spatial_size, hist_bins)\n",
    "    for i in patch_list:\n",
    "        cv2.rectangle(image,(i[0], i[1]),(i[2],i[3]),(0,0,255),6)\n",
    "    # Return image with positiv-patches drawn on it and the list with all positiv-patches\n",
    "    return image, patch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choosing parameters used for where to search vehicles in general.\n",
    "ystart = 380\n",
    "ystop = 656"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading the Video and get all frames with the iter_frames function\n",
    "clip = VideoFileClip('/media/frank/Zusatz/CarND-Vehicle-Detection-master/project_video.mp4')\n",
    "clip_frames = clip.iter_frames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using parallelization to use all 4 cpu-kernels\n",
    "# This is not necessary, but it takes a really long time (hours) to search in all patches for vehicles without a gpu\n",
    "# So to get things slightly faster done I took this route.\n",
    "count = 0\n",
    "\n",
    "def help_function(clip_frame):\n",
    "    global count\n",
    "    print(count)\n",
    "    print(datetime.datetime.now().time())\n",
    "    count = count+1\n",
    "    output_carsearch = find_cars_range(clip_frame)\n",
    "    return output_carsearch[1]\n",
    "\n",
    "pool = multiprocessing.Pool(processes=4)    \n",
    "results = pool.imap(help_function, clip_frames)\n",
    "pool.close()\n",
    "pool.join()\n",
    "patch_list=[]\n",
    "for result in results:\n",
    "    patch_list.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I pickle the list so I don't have to search for cars in the video anew\n",
    "# This has saved me a lot of time...\n",
    "with open('list.pkl', 'wb') as f:\n",
    "    pickle.dump(patch_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('list.pkl', 'rb') as f:\n",
    "    load_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load_list = patch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To get rid of false positives I make a new list for the multiple detection suggested from udacity\n",
    "list1 = list(load_list)\n",
    "list2 = list(load_list)\n",
    "list3 = list(load_list)\n",
    "list4 = list(load_list)\n",
    "list5 = list(load_list)\n",
    "\n",
    "list1.insert(0, []), list1.insert(0, []), list1.insert(0, []), list1.insert(0, [])\n",
    "list2.append([]), list2.insert(0, []), list2.insert(0, []), list2.insert(0, [])\n",
    "list3.append([]), list3.append([]), list3.insert(0, []), list3.insert(0, [])\n",
    "list4.append([]), list4.append([]), list4.append([]), list4.insert(0, [])\n",
    "list5.append([]), list5.append([]), list5.append([]), list5.append([])\n",
    "\n",
    "whole_liste = []\n",
    "for i in range(len(list3)):\n",
    "    whole_liste.append(list1[i]+list2[i]+list3[i]+list4[i]+list5[i])\n",
    "del whole_liste[0]\n",
    "del whole_liste[0]\n",
    "del whole_liste[-1]\n",
    "# See last cell to see why the next line is outcommented\n",
    "#del whole_liste[-1]\n",
    "print(len(whole_liste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Using heatmaps and labels to get a patch for each car on the frame\n",
    "# (taken from udacity-lessons and altered)\n",
    "counter = 0\n",
    "def video_pipeline(image):\n",
    "    # There is surely a better way than using global parameters (never a good idea)\n",
    "    global counter\n",
    "    global whole_liste\n",
    "    heat = np.zeros_like(image[:,:,0])\n",
    "    list_patch = whole_liste[counter]\n",
    "\n",
    "    counter = counter+1\n",
    "    if list_patch != []:\n",
    "        for i in list_patch:\n",
    "            heat[i[1]:i[3], i[0]:i[2]] += 1\n",
    "        # Choosing heat threshold\n",
    "        heat[heat <= 11] = 0\n",
    "        heat = np.clip(heat, 0, 1)\n",
    "        labels = label(heat)\n",
    "        for car_number in range(1, labels[1]+1):\n",
    "            nonzero = (labels[0] == car_number).nonzero()\n",
    "            nonzeroy = np.array(nonzero[0])\n",
    "            nonzerox = np.array(nonzero[1])\n",
    "            bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "            cv2.rectangle(image,bbox[0],bbox[1],(0,0,255),6)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using the function on the project_video.mp4 and making a new video output.mp4\n",
    "output_video = 'output.mp4'\n",
    "clip = VideoFileClip('/media/frank/Zusatz/CarND-Vehicle-Detection-master/project_video.mp4')\n",
    "\n",
    "# For some reason the function write_videofile do not take the framerate (25) from the clip in, but\n",
    "# just use the standard (24). So I have to set the parameter myself to 25, but now he gives me 1261 frames\n",
    "# instead of 1260 (length of whole_list). Therefore I commented del whole_list out some cells above. \n",
    "# Not the best workaround... Have to find out why write_videofile does that so badly - maybe I will switch to pure ffmpeg\n",
    "# or opencv-videowriting in the future, but for now it have to do.\n",
    "output_clip = clip.fl_image(video_pipeline)\n",
    "%time output_clip.write_videofile(output_video, audio=False, fps=25)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"1280\" height=\"720\" controls>\n",
    "   <source src=\"{0}\">\n",
    "<\\video>\n",
    "\"\"\".format(output_video))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:CarND-LeNet-Lab]",
   "language": "python",
   "name": "conda-env-CarND-LeNet-Lab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}